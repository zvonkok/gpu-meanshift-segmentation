%!TEX TS-program = pdflatex
%!TEX encoding = UTF-8 Unicode


\chapter{Optimization Strategies}
\label{ch:optimization}


The following chapter will present optimization strategies which were used to
accelerate the mean shift image segmentation. The presented strategies are not
only valid for \gls{CUDA}, they can be applied to many parallel machines which
are build after the shared memory or \gls{SPMD} model. The chapter is divided in
several sections. Each section will present an optimization. The are major and
minor optimization that are examined where mostly major optimization give higher
speedup whereas minor optimization give lower speedup. 

Many of the optimization are hardware centric, which means that one has to have
a good knowledge about the architecture to understand which steps can be
undertaken to accelerate the algorithm and which where even possible. Each
section will cover the architectural specialty which led to perform the specific
optimization. The first optimization of the next section is a general rule when
trying to accelerate an algorithm, offload compute intensive parts. The succeeding
sections respectively optimizations are in chronological order and where applied
in that order, identifiable by the ever increasing speedups. 


\section{Test \textit{\&} Benchmark Configuration} % (fold)
\label{sec:test__benchmark_configuration}
For measuring the run-time the \gls{CUDA} timers are used. They have a
resolution of approximately half a microsecond. The timings are measured on the
\gls{GPU}, and hence they are operating system independent. Further benchmark
metrics like bandwidth, coalesced or uncoalesced access to global,shared and
local memory or time spent in device functions were reported by cudaprof a
profiler for \gls{CUDA} programs. OProfile was used on the host side for
profiling the program.

For the measurement a image with a dimension of 256$\times$256 pixel was used
and with the following parameters: \textsf{sigmaS} = 7, \textsf{sigmaR} = 6.5
and \textsf{minRegion} = 20 (\textsf{minRegion} is used for pruning regions
smaller than 20 pixel).
% section test_\&_benchmark_configuration (end)


\section{Offload Compute Intensive Parts}
\label{sec:offload_intensive}
\fpAdd{\timegold}{0,0}{9616,77}
\fpAdd{\timecuda}{0,0}{1932}
\fpDiv{\speedup}{\timegold}{\timecuda}

As shown in \autoref{ch:algorithm_analysis} {\color{red} DESIGN IMPLEMNtAtION..}
the most compute intensive part of the mean shift segmentation is the filtering
step. bla bla blbub.. mal sehen was vorne noch steht 98\%. In this case the most
compute intensive part is 98.2\% of the complete run-time and hence a valuable
part to offload. All other parts have no significant part of the run-time but
when optimizing it can happen that these other part grow in terms of run-time
and the before most compute intensive part with a very high percentage drops to
a low percentage and hence the percentages of all other parts increase and
become very compute intensive. 

In other cases the run-time is often distributed among several parts and in this
case it should be tried to offload as much as possible of the computational parts
to an accelerator as the \gls{GPU}. In the first place the focus will be on the
filter part that is executing in total 99.1\% of the run-time.

For the following considerations about speedup the \autoref{eq:speedup} will be
used to calculate the speedup. 

In {\color{red} bezug zu design implementierun} it was shown how to design and
to implement the algorithm to offload the important parts to the \gls{GPU} and
the first naive implementation of the mean shift filter in \gls{CUDA} resulted
in a speedup of

\begin{equation*}\label{eq:speedup0}
	S = \frac{T_{CPU}}{T_{GPU}} = %
	\frac{\unit[\timegold]{ms}}{\unit[\timecuda]{ms}} = \speedup
\end{equation*}

times faster than the \gls{CPU} version. The reference time was generated from
\href{http://www.caip.rutgers.edu/riul/research/code.html}{ \gls{EDISON}}. It
reports timings for filtering and segmentation.
The first implementation was just a naive implementation without regarding the
hardware restrictions and without exhausting the hardware potential. In
\autoref{sec:data_flow} it was shown that in every iteration of calculating the
mean shift vector the algorithm fetches color values for every pixel contained
in the search window. The search window is defined by \textsf{sigmaS} the
spatial window and \textsf{sigmaR} the range window. For now, only
\textsf{sigmaS} is important since that value spans the spatial window over
the pixel (see \autoref{lst:msk}). 



\section{Global Memory \textit{\&} Coalescing} 
\label{sec:coalescing}
In \autoref{sec:data_flow} it was shown that the search window is moving
unpredictable over the image the main memory is accessed randomly. The 
issue that raises up is that only a fraction of the bandwidth is utilized. 
Therefore it is advisable to clubb or coalesce the access to the
global memory. 

The \gls{GPU} can coalesce global memory reads and wirtes in as few as one
transaction, or two transactions in the case of 128-bit words when certain
access requirements are met. These requirements are e.g. that a specific
access pattern is realized, the address of a 64-byte segment is 64-byte aligned 
and further more. For a complete list see Chapter 3 of \citep{citeulike:6584051}.
This requirements are typical for high speed memory not only valid for \glspl{GPU}
rather for many parallel architectures like the Cell or the Niagara \gls{CPU}. 

To satisfy one requirement, the access pattern, it is advisable to use the
\emph{float4} data type. It ensures that data is accessed on consecutive
memory locations and hence it can be grouped together. After rewriting the 
algorithm the new speedup was 
\fpAdd{\timecuda}{0,0}{1454}
\fpDiv{\speedup}{\timegold}{\timecuda}
\begin{equation*}\label{eq:speedup1}
	S = \frac{\unit[\timegold]{ms}}{\unit[\timecuda]{ms}} = \speedup
\end{equation*}

The speedup was really not very satisfying but the \gls{GPU} has another mechanism
which could be used to speedup the algorithm. In {\color{red} LINK ZU read only image data}
it was shown that the input image is read only and the results are written to another
structure. Furthermore read pixels from the image are reused in the calculation
when the search window is shifted and the access is rather random. 

What now proves advantageous is the ability of \gls{CUDA} to use texture memory
for computing. Texture memory can be more faster than device memory because it is
first of all read only and it uses a cache for faster access. The image data
stored in a texture is optimized in that way that locality is exploited. The
texture is perfect match for the algorithm as random access do not harm the bandwidth
so much compared to global memory and pixels read in for one search window can 
be reused for the next search window as they are cached. 

Switching to \emph{textures} (glossary texturess) yield to a huge speedup of 
\fpAdd{\timecuda}{0,0}{250}
\fpDiv{\speedup}{\timegold}{\timecuda}
\begin{equation*}\label{eq:speedup2}
	S = \frac{\unit[\timegold]{ms}}{\unit[\timecuda]{ms}} = \speedup
\end{equation*}

Texture memory is great for algorithms were random access to memory occurs and
data read in are used more often than once. Where it is predictable how much and 
how (access pattern) the data is read in global memory in conjunction with shared
memory can be advantegous. But as said at the beginning of this section certain
requirements have to be met to achieve a high bandwidth. 

\section{Division Instruction Optimization}
\label{sec:expensive_divisions}
Every hardware instruction is taking a specific amount of clock cycles to complete. 
For single-precision floating-point arithmetic instructions like add, multiply
or multiply-add take 4 clock cycle to complete. Whereas the single-precision
floating-point division takes about 36 clock cycles, which is 9$\times$ slower 
than the common arithmetic instructions. Therefore divisions should be avoided
as much as possible to increase instruction throughput. 


Having a look at \autoref{alg:msf} it can be seen that in the calculation of the
mean shift vector divisions occur. The denominator is the size of the search
window. In this implementation the size of the search window is not changing and
hence a constant. This circumstance can be exploited and the reciprocal can be
calculated. The reciprocal can then be substituted by the division and denominator
to produce faster multiplications.

For example if it looks like this
\begin{lstlisting}[caption=Expensive divison, label=lst:division]
// Determine if inside search window
// Calculate distance squared of sub-space s
float dl = (luv.x - yj_2) / sigmaR;               
float du = (luv.y - yj_3) / sigmaR;               
float dv = (luv.z - yj_4) / sigmaR;
\end{lstlisting}
one can calculate \textsf{rsigmaR} = 1.0f/\textsf{sigmfaR} in advance and everywhere where a
division by \textsf{sigmaR} occurs replace it into a multiplication. The
resulting code is then
\begin{lstlisting}[caption=Division turned into fast multiplication, label=lst:precalcdivision]
// Determine if inside search window
// Calculate distance squared of sub-space s
float dl = (luv.x - yj_2) * rsigmaR;               
float du = (luv.y - yj_3) * rsigmaR;               
float dv = (luv.z - yj_4) * rsigmaR;
\end{lstlisting}

With this technique the kernel code is free of divisions except only one that
cannot be eliminated since the value is changing throughout the calculations.
This optimization yielded a speedup of

\fpAdd{\timecuda}{0,0}{193}
\fpDiv{\speedup}{\timegold}{\timecuda}
\begin{equation*}\label{eq:speedup3}
	S = \frac{\unit[\timegold]{ms}}{\unit[\timecuda]{ms}} = \speedup
\end{equation*}


\section{Execution Configurations} % (fold)
\label{sec:run_configurations}
It is important to check the best execution configuration. Each execution
configuration has its benefit. Some can exploit the bandwidth to the global
memory some can benefit from the cache of the texture. Furthermore it is
important to keep the hardware busy to hide memory latencies. Busy means that
one should design the application to use threads and blocks so that the
hardware can balance the work across the multiprocessors.

A good execution configuration can hide latency arising from register
dependencies , provide optimal computing efficiency and facilitate coalescing to
global memory. Not every execution configuration is possible because ressources
are limited and too many threads can eat up one ressource where the other ressource
is not used. The ressource meant here are the shared, local memory and the register
usage. See the \gls{CUDA} programming guide \citep{citeulike:3325943} for a in 
depth description about the effects of execution configuration.

Several test showed that the best execution configuration for this algorithm is
2$\times$64 threads and the new speedup  resulting out of this new configuration
was

\fpAdd{\timecuda}{0,0}{171}
\fpDiv{\speedup}{\timegold}{\timecuda}
\begin{equation*}\label{eq:speedup4}
	S = \frac{\unit[\timegold]{ms}}{\unit[\timecuda]{ms}} = \speedup
\end{equation*}
 Another positive effect of a configuration that is multiple of the warp size is
that the scheduler can work more efficiently because a warp consist out of 32 
threads. 
% section run_configurations (end)

\section{Native Data Types}
The \gls{CUDA} Programming Guide \citep{citeulike:3325943} highly recommends the
use of the \emph{float} type for single-precision floating-point code.
Additionally it is recommended to use the single-precision floating-point
mathematical functions. One should use the $float$ data type where ever
possible. The \glspl{GPU} are highly optimized for floating point calculations.
After converting all integer calculations to floating point operations another
speedup was achieved.
\fpAdd{\timecuda}{0,0}{165}
\fpDiv{\speedup}{\timegold}{\timecuda}
\begin{equation*}\label{eq:speedup5}
	S = \frac{\unit[\timegold]{ms}}{\unit[\timecuda]{ms}} = \speedup
\end{equation*}
On current \gls{GPU} hardware integer operations can be very expensive in terms
of clock cycles. The situation can change with future \gls{GPU} architecture check
against \emph{Fermi} the new architecture from {\slcsmallcaps{NVIDIA}} 
\autoref{citeulike:6640188}.



\section{Avoid Branch Divergence} % (fold)
\label{sec:avoid_branch_divergence}
On a architecture with such high throughput of calculations per cycle it is
preferred to calculate values rather then generating them through $if$ and
$else$ statements. Further problems arise when control flow depends on the
thread id which means that they are divided into groups that are executing
different execution paths. In this case the execution gets serialised. This can
happen when a thread has found a density maximum and all the other threads are
still calculating. When the active threads finish the calculation they converge
back to the same execution path as the idle threads and they finish together.

Therefore the remaining $if$ and $else$ statements where arranged in such a way
so that a thread is exiting early from the loop and avoiding unnecessary
calculations. This leads of course to higher branch divergence but the execution
time is lower. 

The speedup achieved from this optimisation was
\fpAdd{\timecuda}{0,0}{115}
\fpDiv{\speedup}{\timegold}{\timecuda}
\begin{equation*}\label{eq:speedup6}
	S = \timegold\ ms/\timecuda\ ms = \speedup
\end{equation*}



\section{Shared Memory} % (fold)
\label{sec:shared_memory}
The optimization guides state that one should use shared memory to avoid 
redundant transfers from global memory. Shared memory is much faster than global
and local memory assumed that there are no bank conflicts. The \gls{GPU} shared
memory is divided into so called banks for simultaenous access. If two threads
are accessing the same bank the access is serialized and hence can be very slow
if not only two but far more threads are accessing the same bank 
\citep{citeulike:6584051}.

In \autoref{sec:data_flow} it was shown that the search window is moving
unpredictable over the image and hence preloading of data into the shared memory
is not possible. Furthermore if two or more search windows from different
threads overlap it is obvious that the access to the pixel would lead to bank
conflicts. After many tryouts this optimization was discarded. 

\section{Know the Algorithm} % (fold)
\label{sec:know_the_algo}
This section is not related to the hardware more related to the software part of 
the algorithm. After all the other optimizations done it was obvious that there
was not more room for many optimizations that target the hardware. The focus went
on to the algorithm and its peculiarity when filtering the image. 

The idea was to examine how many iterations in average the algorithm needs to
filter the image. In the original code of \gls{EDISON} there is variable named
\textsf{iterationCount} which limits the iteration count to 100 per pixel. The
comment in the source code says \textit{``Iteration count is for speed up
purposes only -- it does not have any theoretical importance"} that brought up a
question, why it is needed anyway when the mean shift algorithm converges
\citep{citeulike:462300}. For this the source code was modified in that way
that for each pixel the iteration count was reported. A precise look a the 
generated numbers showed that some pixel have iteration counts of 100. 

To have an clue if its worth to continue the research into this direction several
tests were done. Looking at the iteration count it sticks out that most of the 
pixel are finished after about 10 iterations, it was strange to see that several
pixel were iterating till 100. 

The approach taken was to limit the iteration count not to 100 but to 10 and 50.
Setting the limit to 10 and executing the \gls{CPU} and \gls{GPU} version the
speedup was 
 
\fpAdd{\tgold}{0}{7369}
\fpAdd{\tcuda}{0}{29}
\fpDiv{\speedupten}{\tgold}{\tcuda}
\begin{equation*}\label{eq:speedup7}
		S = \frac{\unit[\tgold]{ms}}{\unit[\tcuda]{ms}} = \speedup
\end{equation*}

Setting the limit to 50 and executing the \gls{CPU} and \gls{GPU} version again
the speedup was
\fpAdd{\tgold}{0}{10373}
\fpAdd{\tcuda}{0}{82}
\fpDiv{\speedup}{\tgold}{\tcuda}
\begin{equation*}\label{eq:speedup8}
	S = \frac{\unit[\tgold]{ms}}{\unit[\tcuda]{ms}} = \speedup
\end{equation*}

In the case where the limit is 10 the \gls{GPU} could achieve a speedup of 
\fpDiv{\speedup}{\tgold}{\tcuda} \speedupten$\times$ because all multiprocessors
are busy then and are not idle as in the case when the limit is 100 and just a few
threads are calculating and the rest idles. The algorithm is as fast as the slowest
thread which is iterating until 100. 

A candidate with iteration count of 100 is picked up from the iteration list and
examined further. For this the magnitude squared (the length) of the mean shift
vector is analysed over the iterations. When the mean shift vector approaches
the convergence point the magnitude becomes smaller and smaller until it falls
under a small number. In this implementation this small number is
$\epsilon=0.01$. Then it can be said that the mean shift vector has reached the
basin of attraction.

\subsubsection{Attractor} % (fold)
\label{ssub:attractor}
Dynamical systems evolve to a specific set called an attractor after many
iterations. Dynamical systems are described either by differential equations or
by difference equations. A difference equation is a specific type of recurrence
relation where each term of a sequence is defined as a function of the
preceding terms.

Such an attractor can be a point (fixed point), curve (limit cycle), manifold or
a complicated set with a fractal structure known as a strange attractor, see
\citeauthor{citeulike:3745535} \citep{citeulike:3745535} for impressive pictures
of strange attractors.

But not only dynamical systems can lead to such an attractor, furthermore the 
effect of quantization, the limited precision of the float data type can lead to
an attractor. 

The above terminology comes from chaos theory where the behaviour of dynamical
and iterative functions systems are described. In \autoref{par:chaos_theory} 
several books and articles were presented for further information. 

The chaos theory was mentioned because it can happen very often without knowledge
that some algorithms are iterating and iterating and the stopping criterion is never
reached. In the mean shift algorithm case the upper bound was set to 100 without
mentioning why. If the algorithm converges an that is proven, why do we have a 
upper limit anyhow? Because in this case there are attractors as well. All the 
densities where the mean shift vector moves to are attractors, fixed points.
Small perturbations of the vector lead to the same density. 

But there are also attractors where the mean shift vector is moving to and it is
not the final density. 
% subsubsection attractor (end)

\begin{lstlisting}[caption=Magnitude squared of pixel 10992, label=lst:msp]
10992 - 0.00000 - 1
10992 - 2.52029 - 2
10992 - 2.52029 - 3
10992 - 2.52029 - 4
10992 - 2.52029 - 5
	ยง$\ldots$ยง
10992 - 2.52029 - 100
\end{lstlisting}
Examining the sequence of the magnitude squared for pixel $10992$ in
\autoref{lst:msp} one can easily see that after some iterations the magnitude is
fixed to $2.52029$. The algorithm is stuck at this value and will not move away
from this attractor. An attractor with a single value is a fixed point. 

At the beginning of the section it was shown that if the iteration count of each
pixel is dropped to a lower level the performance increases. Further examination
showed that several pixel have attractors that were limit cycles. Which means that
the calculation of the magnitude squared is periodic. The period in this case
is from a period-2 up to a period-8 limit cycle.

For visual feedback the source image in \autoref{fig:orig} was taken and for
each pixel the iteration count was visualized. The \autoref{fig:lc} shows
from blue to red, where red values represent high iteration count, the iterations
for each pixel in a 128$\times$128 big picture.

\begin{figure}[ht] \centering
	\begin{tikzpicture} 
		\begin{axis}[
				height=5cm,
				width=0.91\textwidth,
			  shader=interp, % interp = faster, smaller pdf's
				point meta max = 100, % color bar range from 0 - 100
				colorbar,
			 	colorbar style={ ymax=100},
%				colormap name = dissmap,
			 	xlabel=$x$, 
				ylabel=$y$,
				zlabel=$z$,
				zmin=0, zmax=100,
				] 
				\addplot3[surf,mesh/rows=128] file {Plots/iter.128.limitcycle0.dat}; 
		\end{axis}
	\end{tikzpicture}
\caption{Iteration count for each pixel}
\label{fig:lc}%
\end{figure}

It can be seen that there a several pixel that ware iterating to 100. To circumvent
this problem a simple limit cycle detection algorithm was implemented to prevent
that several threads are blocking the termination of the algorithm. 

In the first place a period-4 limit cycle detection algorithm was implented where
a vector was saved with the last four values and compared to the new calculated 
value. If any value from the vector matches the new value the thread terminates
and finishes the computation. 

The period-4 limit cycle detection led to a speedup of 
\fpAdd{\timecuda}{0,0}{99}
\fpDiv{\speedup}{\timegold}{\timecuda}
\begin{equation*}\label{eq:speedup9}
	S = \timegold\ ms/\timecuda\ ms = \speedup
\end{equation*}
Extending the limit cycle detection to period-8 limit cycle detection a speedup
of 
\fpAdd{\timecuda}{0,0}{87}
\fpDiv{\speedup}{\timegold}{\timecuda}
\begin{equation*}\label{eq:speedup10}
	S = \timegold\ ms/\timecuda\ ms = \speedup
\end{equation*}
was achieved. Surely one cannot be sure to have limit cycles with a higher
period but the first problem is the register pressure. Because the compiler has
to keep the saved values of the previous iterations in the registers for
comparison the registers cannot be used for the other calculations. The
\gls{CUDA} compiler tries to keep as much as possible in the registers. Secondly
comparisons are not very fast as computation and having to compare 16 or even 32
values makes no sense because than the prevention of limit cycles would cost
more performance than just iterating to 100. The following series of
\autoref{fig:lc0},\autoref{fig:lc1}, \autoref{fig:lc2} show visually the effect
of limit cycle detection an how all iterations to 100 vanish.

\begin{figure}[ht] \centering
	\begin{tikzpicture} 
		\begin{axis}[
				height=5cm,
				width=0.91\textwidth,
			  shader=interp, % interp = faster, smaller pdf's
				point meta max = 100, % color bar range from 0 - 100
				colorbar,
			 	colorbar style={ ymax=100},
%				colormap name = dissmap,
			 	xlabel=$x$, 
				ylabel=$y$,
				zlabel=$z$,
				zmin=0, zmax=100,
				] 
				\addplot3[surf,mesh/rows=128] file {Plots/iter.128.limitcycle0.dat}; 
		\end{axis}
	\end{tikzpicture}
\caption{Period-0 limit cycle detection}
\label{fig:lc0}%
\end{figure}

\begin{figure}[ht] \centering
	\begin{tikzpicture} 
		\begin{axis}[
				height=5cm,
				width=0.91\textwidth,
			  shader=interp, % interp = faster, smaller pdf's
				point meta max = 100, % color bar range from 0 - 100
				colorbar,
			 	colorbar style={ ymax=100},
%				colormap name = dissmap,
			 	xlabel=$x$, 
				ylabel=$y$,
				zlabel=$z$,
				zmin=0, zmax=100,
				] 
				\addplot3[surf,mesh/rows=128] file {Plots/iter.128.limitcycle4.dat}; 
		\end{axis}
	\end{tikzpicture}
\caption{Period-4 limit cycle detection}
\label{fig:lc1}%
\end{figure}


\begin{figure}[ht] \centering
	\begin{tikzpicture} 
		\begin{axis}[
				height=5cm,
				width=0.91\textwidth,
			  shader=interp, % interp = faster, smaller pdf's
				point meta max = 100, % color bar range from 0 - 100
				colorbar,
			 	colorbar style={ ymax=100},
%				colormap name = dissmap,
			 	xlabel=$x$, 
				ylabel=$y$,
				zlabel=$z$,
				zmin=0, zmax=100,
				] 
				\addplot3[surf,mesh/rows=128] file {Plots/iter.128.limitcycle8.dat}; 
		\end{axis}
	\end{tikzpicture}
\caption{Period-8 limit cycle detection}
\label{fig:lc2}%
\end{figure}

\section{Unrolling Loops and Multiplications} % (fold)
\label{sec:unrolling_loops_and_multiplications}
The last improvement that was made is to unroll loops and complicated
multiplications. The idea behind this is that the compiler has more chances to
schedule the instructions and reorder them in a proper way. This is important
because there are some instructions, e.g. \textsf{fmadd}, that are executing two
floating point operations rather than just one floating point operation per 4
clock cycles and unrolled multiplications are more \emph{clear} to the compiler. 

After unrolling the multiplications and loops a final speedup of 
\fpAdd{\timecuda}{0,0}{73}
\fpDiv{\speedup}{\timegold}{\timecuda}
\begin{equation*}\label{eq:speedup11}
	S = \timegold\ ms/\timecuda\ ms = \speedup
\end{equation*}
was achieved.

\section{Summary} % (fold)
\label{sec:optimization_summary}
The naive implementation has already led to an speedup of 5$\times$. But putting
some knowledge and energy into the optimization one can acheive a speedup of 
\speedup$\times$. The optimization was not only done on hardware but also on the 
algorithm side. It is not enough to know some programming langauge very good the
hardware on which its run is more important. No programming langauge will expose
all hardware features directly to the developer, the developer has to know
about the hardware and its features. Furthermore it is crucial to understand
the data and how it is accessed. The biggest speedup was achieved when
going from global memory to texture memory. The access of data fits perfectly
the texture memory and hence the algorithm can take full advantage of it. 

After all the optimizations and a recent speedup it is now important to look 
after the performance and scalablility of the algorithm. The next chapter is 
going to tackle this. 
% section summary (end)




