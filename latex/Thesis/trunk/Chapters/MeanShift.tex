%*******************************************************************************
\chapter{Mean Shift}\label{ch:mean_shift}
%*******************************************************************************
In low level computer vision tasks like segmentation, edge detection or smoothing
the analysis of data is often not done on the original images. Features like 
colors are rather projected into a feature space where they can be more easily
analyzed. The analysis of the feature space can find interesting attributes of 
the image like edges or segments.

The feature space has to be smoothed before analysis. Feature spaces originate
from real images therefore they are composed of several components from
different distributions. The basic approach of a mixture model, a mixture model
is a probabilistic model for density estimation using a mixture distribution, is
not efficient enough to estimate the density satisfactorily of such complex,
arbitrarily comprised densities. The discontinuity preserving smoothing is therefore
accomplished with kernel based density estimators. Kernel based density estimators
are making no assumptions about densities and hence can be estimate aribtrary 
densities.

The maxima of a feature space correspond to the searched components like edges
of an image. Gradient based methods of feature space analysis are using gradients
of the probability density function to find the maxima. Such methods are complex
because they need among other things a estimation of the probability density.

Mean shift is an alternative to the gradient based methods as it is easier to 
calculate then to estimate the probability density and then to calculate the 
gradient. The mean shift vector points to the same direction as the gradient of
gradient based methods. Furthermore the mean shift vector has a adaptive size and
is non parametric. There is no need to supply a step size compared to the other
methods. Mean shift a robust approach toward feature space analysiswas
originally introduced by \citeauthor{citeulike:462300} \citep{citeulike:462300}.

\section{Density Estimation} % (fold)
\label{sec:density_estimation}
In probability and statistics it is known that for different tasks there exist 
more or less suitable features. In 
\citep{citeulike:167581} \citeauthor{citeulike:167581}  are giving an example of 
a classification of two different fish types. Features like the length and 
brightness are there observed that fit for the task. It is of course possible 
to find more descriptive features for fish like the amount of finns but in 
image processing it would be very expensive and difficult to count. In image
analysis specially in real time applications it is important to find suitable
features for the task, but also features which are easily visually identified. 
Color observations are because of their simplicity and for the eye easily to 
gather important features. The color features can have components from the 
\gls{RGB} or gray value color space. Furthermore there are several other color
spaces that could be observed like the \gls{HSV} or the \gls{Luv} color space
with one luminance and two chromatic components. The \gls{Luv} color space is
often used in computer graphics because of its attribute to be a perceptually
uniform color space.

With a finite set of observations follows a finite feature space. The main point
of \emph{mean shift} is to find the maxima in the feature space. The maxima of a
feature space are all important for the \emph{mean shift} applications
(filtering, segmentation, ...) as the distributions or discontinuities map to
clusters or edges of the image.

The \emph{mean shift} method is based on the gradient method. For the gradient
estimation a function is upon estimations of discrete observations in the
feature space. For this kernel density estimators are used also known as 
\emph{Parzen Window} method.


\section{Kernel Density Estimation} % (fold)
\label{sec:kernel_density_estimation}
Kernel density estimation is a method to estimate an unkown density
distribution with finite observations of a point in the sample space. 
The result of such a procedure is a probability density function that describes 
the density of probability at each point in the sample space. To estimate the 
density of a point $x = { x_1, ... , x_d, ... , x_D} \in \mathbb{R}^D$ in a 
$D$ dimensionl feature space, $N$ observations $x_1^N$ with
 $x_N \in \mathbb{R}^D$ within a search window that is centered around point $x$
have to be observed. The search window with radius $h$ is the bandwidth of the
used kernel. The density of probability in point $x$ is the mean of density of
{\color{iRed}probabilities falling into the search window.}

The effect of different bandwith parameters $h$ (search window radius) is shown in 
\autoref{fig:window_radius}. The example shows a kernel density estimation with
five observations $x = 4, 1, -1, -3, -3.5$ and a normal kernel. The total density
estimation is the sum of each kernel at a observation, here shown for three 
bandwidths. With bigger bandwidth $h$ the density estimation becomes smoother.  

\begin{figure}[ht]
\centering
%\includegraphics[width=0.6\textwidth]{gfx/itr_limitcycle0.pdf}
\caption{Effect of bandwidth selection}
\label{fig:window_radius}
\end{figure}


Kernel density estimation is a non parametric method, although some parameters
exists like the search window radius. Non parametric methods are making no 
assumptions about the density of probabilities. The strenght of such methods is
that they are not limited to just on probability but can deal with arbitrary
coupled/joined probabilities. With an unfinite number of observations the non
parametric methods can reconstruct the density of the original
probabilities.

To derive the kernel density estimator in point $x \in \mathbb{R}^D$ first of
all some definitions have to be made. Let $x$ be a random variable and $N$
observations $x_1^N$ with $x_n \in \mathbb{R}^D$ given. The kernel density
estimator $\hat(f)(x)$ in a point $x \in \mathbb{R}^D$, with a kernel $K(x)$ and a
$DxD$ bandwith matrix $\mathbf{H}$ is 
\begin{equation}\label{eq:kernel0}
	\hat{f}(x) = \frac{1}{N} \sum_{n = 1}^N K_{\mathbf{H}}\left( x-x_n \right)
\end{equation} 
where
\begin{equation}\label{eq:kernel1}
	K_{\mathbf{H}}(x) = \frac{1}{\sqrt{|\mathbf{H}|}}K \left( \frac{x - x_n}{h} \right)
\end{equation} 

Since a full parametrized matrix $\mathbf{H}$ would lead to very complex estimates
only a single bandwidth parameter, the window radius will be regarded. With the
simplification $\mathbf{H} = h^2 \mathbf{I}$ the \autoref{eq:kernel0} can be 
written as
\begin{equation}\label{eq:kernel2}
	\hat{f}(x) = \frac{1}{Nh^D}\sum_{n=1}^N K\left( \frac{x - x_i}{h} \right).
\end{equation}

The kernel density estimator is valid for several kernels and successive
considerations will deal with several kernel the \autoref{eq:kernel2} will be
formated into a more generic form. For this transformation the definition of a
kernel and profile of the kernel is needed. The following definition of a kernel
and its profile is from \citeauthor{citeulike:2522867}
\citep{citeulike:2522867}. The norm $\lVert x \rVert$ of $x$ is a non negative number 
so that $\lVert x \rVert^2 = \sum_{d = 1}^D|x_d|^2$.
A $K:\mathbb{R}^D \rightarrow \mathbb{R}$ is
a known as a kernel, when there is a function $k:[0, \infty] \rightarrow
\mathbb{R}$ the profile of the kernel, so that
\begin{equation}\label{eq:kernel3}
	K(x)=c_{k,d}k(\lVert x \rVert^2)
\end{equation}
where $K$ is radial symmetric, $k$ is non negativ {\color{iRed} nicht zunehmend
und stückweise stetig ist} with $\int_0^{\infty} k(r) dr < \infty$. $c_{k,D}$ is
a positive normalization constant so that $K(x)$ integrates to $1$. 

Now the kernel density estimator from \autoref{eq:kernel2} can be transformed into
a new equation. The two indices $K$ and $h$ are representing which kernel and 
which radius are used for the density estimator. With the profile notation $k$, where
\autoref{eq:kernel2} is inserted into \autoref{eq:kernel1}, the \autoref{eq:kernel2}
is transformed to
\begin{equation}\label{eq:kernel4}
	\hat{f}_{h,K}(x) = \frac{c_{k,D}}{Nh^d}
	\sum_{n = 1}^N k\left(\left\lVert \frac{x-x_n}{h} \right\rVert^2\right)
\end{equation}



\section{Kernel and their Properties} % (fold)
\label{sec:kernel_properties}
The following section will introduce three univariate profiles and their associated
multivariate radial symmetric kernels. 

From the Epanechnikov profile
\begin{equation}\label{eq:epa}
	k_E{x} = \begin{cases}
				1 - x &  0 \leq x \leq 1\\
				0 & x > 1
		\end{cases}, x \in \mathbb{R}
\end{equation}
follows a radial symmetric kernel
\begin{equation}\label{eq:epak}
	K_E(x) = \begin{cases}
				\frac{1}{2}c_D^{-1}(D+2)(1-\lVert x \rVert^2) &  \lVert x \rVert \leq 1 \\
				0 & \mathit{otherwise}
		\end{cases}, x \in \mathbb{R}^D
\end{equation}
 
where $c_D$ is the Volume of the $D$ dimensional globe. The epanechnikov kernel
is used often as it minimizes the \gls{MISE} \citep{citeulike:5813637}. The
\autoref{fig:kernels} (a) shows the epanechnikov kernel. The dervative of the 
kernel is a uniform profile.



\begin{figure}[ht]
\centering
%\includegraphics[width=0.6\textwidth]{gfx/itr_limitcycle0.pdf}
\caption{Kernel density estimators}
\label{fig:kernels}
\end{figure}

From the normal profile
\begin{equation}\label{eq:nml}
	k_N(x) = exp \left( - \frac{1}{2} x \right) \textrm{ where } x \geq 0, x \in \mathbb{R}
\end{equation}
follows the normal kernel
\begin{equation}\label{eq:nmlk}
	K_N(x) = (2\pi)^{-D/2}exp\left( -\frac{1}{2} \lVert x \rVert^2 \right), x \in \mathbb{R}.
\end{equation}
The normal distribution as every other kernel with infinite support, is often
capped for finite support. Finite support is important for the convergence. Capping
the normal kernel can be accomplished by multipyling it by a uniform kernel where
the inner part of the normal kernel is cut out and weighted with $1$ and the 
outer part is set to $0$. The derivative of the normal profile is again a 
normal profile.

From the uniform profile 
\begin{equation}\label{eq:unf}
	k_U(x) = \begin{cases}
				1 &  0 \leq x \leq 1\\
				0 & \mathit{otherwise}
		\end{cases}, x \in \mathbb{R}
\end{equation}
follows the uniform kernel
\begin{equation}\label{eq:unfk}
	K_U(x) = \begin{cases}
				1 &  \lVert x \rVert  \leq 1\\
				0 & \mathit{otherwise}
		\end{cases}, x \in \mathbb{R}^D
\end{equation}

which is a {\color{iRed}hyper uniform globe} in the origin. 

Asuming that a derivative of a profile $k(x)$ exists for all $x \in [0, \infty)$, 
follows a new profile $g(x)$. Now a new kernel $G(x)$ can be defined as 
\begin{equation}\label{eq:shadowk}
	G(x) = c_{g,D}g(\lVert x \rVert^2)
\end{equation}
where $c_{g,D}$ a normalizing constant and $K(x)$ the \emph{shadow kernel} of 
$G(x)$. The term \emph{shadow kernel} was introduced in the context of mean 
shift in \citep{citeulike:2522867}. The mean shift vector of a kernel points to 
the same direction as the gradient of the shadow kernel (See \autoref{sub:gradient_estimator}). 


\section{Mean Shift} % (fold)
\label{sec:mean_shift}
In gradient based methods first the gradient s calculated and then the kernel is
shifted by a vector with a specific length in direction of a maxium of the 
probability. The magnitude/length that is the step size of the vector has to be
chosen. The problem of such gradient based methods is the choice of a suitable
step size. The run time of such algorithms depends heavily on the right choice
of the step size. If the step size is too large the algorithm diverges and 
choosing a too small step size the algorithm becomes very slow. Convergence is
only guaranteed for infinitesimal step sizes. There are several complex 
procedures for finding the right step size, see \citeauthor{citeulike:462300} 
\citep{citeulike:462300}.

In the case of mean shift there are no additional procedures needed to choose
the step size. The magnitude/lenght of the mean shift vector is the step size 
which is adaptive regarding the local gradient of the density of probability. 
Because of this adaptive nature the mean shift algorithm converges (Proof see 
\citeauthor{citeulike:462300} \citep{citeulike:462300}).

The advantage of the mean shift method contrary to gradient based methods is
that the step size has not to be chosen by hand and the gradient has not to be
calculated. It can be shown that the mean shift vector is pointing to the same
direction as the gradient and that it moves along the gradient to the maxima 
that can be seen in \autoref{eq:ms7}. Hence it is sufficient to calculate the
more efficient mean shift vector rather than the gradient. 

Given are $N$ $D$-dimensional observer feature vectors $x_1^N$ with $x_n \in
\mathbb{R}^D$ and a kernel $G$ at the point
$x = {x_1, ... , x_d, ... , x_D} \in \mathbb{R}^D$ in the feature space with 
search window radius $h$ is 
\begin{equation}\label{eq:ms0}
	m_{h,G}(x) = \frac{\sum_{i=1}^n x_i g\left(\left\lVert \frac{x - x_i}{h}
	\right\rVert^2\right)}{\sum_{i=1}^n g\left(\left\lVert \frac{x - x_i}{h}
	\right\rVert^2\right)} -x
\end{equation}
the $D$-dimensional \textit{mean shift} vector. The $N$ observations are
weighted by the means of kernel $G$, summed and normalized with the total sum.
The \emph{shift} is the difference between the weighted \emph{mean} and $x$, thus
the name \emph{mean shift}. 

As pointed out in \autoref{sec:kernel_density_estimation} the main objective of
density estimation is to efficiently find the maxima of a distribution in
feature space. The maxima of a function $f$ are located at the positions where
the gradient $\nabla f(x) = 0$. With the above stated attribute that the \emph{mean shift} vector
always moves along the direction of the gradient, it is a elegant solution for 
finding the maxima of a distribution without estimating the density.

\subsection{Density Gradient Estimation} % (fold)
\label{sub:gradient_estimation}
The usage of a differentiable kernel alllows one to write the density gradient 
estimator as the gradient of the density estimator

\begin{equation}\label{eq:gradest}
	\hat{\nabla} f_{h,K}(x) \equiv \nabla \hat{f}_{h,K}(x) = 
	\frac{2_{C_{k,D}}}{Nh^{D+2}}%
	\sum_{n = 1}^N (x - x_n)k' 
	\left( \left\lVert \frac{x - x_n}{h} \right\rVert^2 \right)
\end{equation}

where the inner part and a part of the prefactor originate from the differentiation
of $k\left( \left\lVert \frac{x - x_n}{h} \right\rVert \right)$

\begin{equation}\label{eq:kerndiff}
	\begin{split}
		\frac{\delta}{\delta x}
		k\left( \left\lVert \frac{x - x_n}{h} \right\rVert^2 \right) & = 
		\left( \left\lVert \frac{x - x_n}{h} \right\rVert \right)'
		k'\left( \left\lVert \frac{x - x_n}{h} \right\rVert^2 \right) \\ & =
		2 \left( x - x_n \right) \left( \frac{1}{h} \right) 
		k'\left( \left\lVert \frac{x - x_n}{h} \right\rVert^2 \right) \\ & =
		\frac{2}{h^2}(x - x_n)
		k'\left( \left\lVert \frac{x - x_n}{h} \right\rVert^2 \right).
	\end{split}
\end{equation}

Using $g(x) = -k'(x)$ and with \autoref{eq:shadowk} a new kernel $G(x)$ with
profile $g(x)$ can be defined. Transforming \autoref{eq:gradest} with the new
profile $g(x)$ the gradient of the density estimator becomes
\begin{subequations}
	\begin{align}
	\hat{\nabla} f_{h,K}(x) & = \frac{2c_{k,D}}{Nh^{D+2}}\sum_{i=1}^N \left(x_n -
    x\right)g\left(\left\lVert \frac{x - x_n}{h} \right\rVert^2\right) \label{eq:gradest1}\\ & 
	= \frac{2c_{k,D}}{Nh^{D+2}} \left[ \sum_{n=1}^N g\left(\left\lVert
	\frac{x - x_n}{h} \right\rVert^2\right) \right] \left[
	\frac{\sum_{i=1}^N x_i g\left(\left\lVert \frac{x - x_n}{h}
	\right\rVert^2\right)}{\sum_{n=1}^N g\left(\left\lVert \frac{x - x_n}{h}
	\right\rVert^2\right)} - x \right].
	\label{eq:gradest2}
	\end{align}
\end{subequations}

The first term of \autoref{eq:gradest2} conforms with the density estimator
$\hat(f)_{h,G}(x)$ for kernel $G$ (compare with \autoref{eq:kernel4} for kernel
$K$) except for a factor whereas the second term is the difference between the
center of the, with kernel $G$ weighted center of observation and the center $x$
of the kernel window which conforms to the \emph{mean shift} vector from 
\autoref{eq:ms0}.
To localize the maxima with \emph{mean shift}, the maxima are the roots of the gradient,
it firstly has to be shown that the \emph{mean shift} vektor is moving along the 
direction of the gradient. Inserting $\hat(f)_{h,G}(x)$ and $m_{h,G}(x)$ into
\autoref{eq:gradest2} follows
\begin{equation}\label{eq:ms1}
	\hat{\nabla} f_{h,K}(x) = \frac{2c_{k,D}}{h^2c_{g,D}} 
	\hat{f}_{h,G}(x)m_{h,G}(x)
\end{equation}
transformed to $m_{h,G}(x)$ follows
\begin{equation}\label{eq:ms2}
	m_{h,G}(x) = \frac{1}{2}h^2 c 
	\frac{\hat{\nabla} f_{h,K}(x)}{\hat{f}_h,G(x)}, \textrm{ whereas } 
	c = \frac{c_{g,D}}{c_{k,D}}.
\end{equation}

The denominator of \autoref{eq:ms2} is the normalizing factor that originates 
from the density estimator with kernel $G$ in $x$ and the numerator is the 
gradient density estimator with kernel $K$. In fact the \emph{mean shift} vector
is proportional to the gradient which means it is adaptive. Kernel $K$ is the 
shadow kernel of kernel $G$. The term shadow kernel was firstly introduced by
\citeauthor{citeulike:2522867} \citep{citeulike:2522867}.

Let be
\begin{equation}\label{eq:msi}
	mi_{h,K}(x) = \frac{\sum_{i=1}^n x_i k\left(\left\lVert \frac{x - x_i}{h}
	\right\rVert^2\right)}{\sum_{i=1}^n k\left(\left\lVert \frac{x - x_i}{h}
	\right\rVert^2\right)} -x
\end{equation}
the $D$-dimensional mean of the observations $x_1^N$ from $\mathbb{R}^D$ 
weighted with kernel $K$ and a window radius $h$. Then is $K$ the 
shadow kernel to kernel $G$ if the \emph{mean shift} vector with kernel $G$
\begin{equation}\label{eq:msi2}	
	m_{h,G}(x) = mi_{h,G}(x) - x = 
	\frac{\sum_{i=1}^n x_i g\left(\left\lVert \frac{x - x_i}{h}
	\right\rVert^2\right)}{\sum_{i=1}^n g\left(\left\lVert \frac{x - x_i}{h}
	\right\rVert^2\right)} -x
\end{equation}
lies in the gradient density estimator direction with kernel $K$
\begin{equation}\label{eq:msi3}
	\hat{f}_{h,K}(x) = \frac{c_{k,D}}{Nh^d}
	\sum_{n = 1}^N k\left(\left\lVert \frac{x-x_n}{h} \right\rVert^2\right)
\end{equation}

In the following sections gradients will not more be considered as it was shown
in \autoref{eq:ms2} that the \emph{mean shift} vector moves along the same
direction as the gradient. Instead of estimating a density with a kernel 
density estimator with kernel $K$ and then calculating the gradient now one can
achieve the same solution with the \emph{mean shift} and the differentiation
$K'$ of kernel $K$. The next section will continue with the actual \emph{mean shift}
method.

\section{Mean Shift Method} 
\label{sec:mean_shift_method}

The \emph{mean shift} vector moves in direction of the maximal slope of the
density, it defines a path to the maximum (Abbildung 4). The \emph{mean shift}
method is described by the iterations in \autoref{lst:ms0}. \\

\begin{lstlisting}[caption=Generic mean shift method, label=lst:ms0, mathescape, numbers=none, escapechar=@, keywordstyle=\color{black}]
  1. Choose a window radius $h_n$ for the kernel density estimator @\autoref{eq:kernel4}@
  2. Choose a start position $y_1 \in \mathbb{R}^D$ for the kernel window
  3. Calculate the @\emph{mean shift}@ vector $m_{h,G}(y_j)$ and shift the kernel window $G$
  4. Repeat 3. until convergence
\end{lstlisting}



\section{Filtering {\upshape\&} Segmentation} % (fold)
\label{sec:filtering_segmentation}
The primary use of \emph{mean shift} is filtering (smoothing) and segmentation. 
A color image can be seen as a 2-dimensional matrix $I \times J$ with $N$ 
3-dimensional vectors. The pixels in the image $x_n, n = 1, ... , N$ consist of
a spatial part $x_n^r = (i,j) \in I \times J$ and a part with a color range
$x_n^f = (r,g,b)$ so $x_n = (x_n^r, x_n^f) \in \mathbb{N}^5$ for $n = 1, ... , N$.
Other color spaces like the \gls{Luv} color space could be used as well. The 
euclidean metric is assumed for both spaces. 

When applying \emph{mean shift} for filtering and segmentaition applications
like in \citeauthor{citeulike:462300} \citep{citeulike:462300} a joined feature
space is used for the spatial and range compoenents of a pixel. The color space
used in filtering and segmentation is the \gls{Luv} color space because of its
feature of linear mapping. In a joint $D$-dimensional feature space ($D=5$ color
images, $D=3$, graytone images) the different attributes of each space have to be
equalized by normalization. Therefore is the joint kernel written as a product of
two kernels with window radius $h_r$ for the spatial part and $h_f$ for the 
color range part
\begin{equation}\label{eq:jokernel}
	K_{h_r, h_f}(x) = \frac{C}{h_r^2h_f^p}
	k \left( \left\lVert \frac{x^r}{h_r} \right\rVert^2\right) 
	k \left( \left\lVert \frac{x^f}{h_f} \right\rVert^2\right) 
\end{equation}
where $p$ is the color dimension of the image, $p=1$ gray tone and $p=3$ color. 
An example of an gray tone image with its feature space is shown in 
\autoref{fig:feature_space}. With the parameter $h = (h_r, h_f)$ the window
radius one can specify the size of the kernel and thereby specifiy the resolution
of the maximum search.

\subsection{Mean Shift Filtering} % (fold)
\label{sub:mean_shift_filtering}
Smoothing or filtering with mean shift or bilateral filters have the advantage
that discontinuity like edges are preserved. Appliying a simple smoothing a 
weighted average of the neighbors in both space and in color range are considered
which systematically excludes pixels across the discontinuity from consideration.

The pixels $x_n, n = 1, ..., N$ of the image converge toward their local density
maximum applying the filtering algorithm \autoref{lst:filtalgo}. \\

\begin{lstlisting}[caption=Mean shift filtering, label=lst:filtalgo, mathescape, numbers=none, escapechar=@, keywordstyle=\color{black}]
  $x_n = (x_n^r, x_n^f), n = 1, ... , N$ are the $D$-dimensional pixels of the
  original image and $z_n, n = 1 , .... , N$ are the $D$-dimensional filtered pixels.

  For each pixel $n = 1, ... , N$
    1. Initialize $k = 1$ and $y_k = x_n$
    2. Calculate $y_{k+1}$ according to @\autoref{eq:msi3}@, repeat until convergence
    3. Set $z_n = (x_n^r, y_{conv}^f)$ 
\end{lstlisting}

All pixels that converge to the same um are lying in the basin of attraction
of this maxium. The filtered image points $z_n, n = 1 , ... , N$ keep there
spatial coordinates but obtain the color values off the convergence point. The
convergence points are found by moving the kernel window with \emph{mean shift} 
in direction of maximum slope of the spatial range feature space. The sample
\autoref{fig:filtsample} was smoothed with the algorithm of \autoref{lst:filtalgo}. 
A uniform kernel with $h = (6.5, 7)$ was used and the method needed ?????
\emph{mean shift} iterations per pixel in the middle. The run time of the filtering
was ??? on a 2.33 \gls{GHz} \gls{CPU}


\subsection{Mean Shift Segmentation} % (fold)
\label{sub:mean_shift_segmentation}






\section{OLD STUFF} % (fold)
\label{sec:old_stuff}

We consider the kernel 
estimator in
$\mathbb{R}^1$. Let $X$ be a random variable. Let $X_i$ where $i=1,2,\ldots,n$ be
% estimate	= schätzen
% density	= dichte
$n$ observations of $X$. We want to estimate the density  of $X$. Let $f$ be the
density of $X$, and $\hat{f}$ an estimator of $f$. A \emph{bin} is an interval
$[a,b)$. $b-a$ is called the width of bin $[a,b)$. Let $O=(0,0)$ be the origin, 
and $[mh, (m+1)h)$ are some bins of constant width $h$, for integers $m$. The
histogram is defined as follows,
\begin{equation}\label{eq:histogram}
	\hat{f}(x)=\frac{1}{nh} \times n_i
\end{equation}
where $n_i$ is the number of $X_i s$ in the same bin as $x$.

The histogram is the oldest and most frequently used density estimator. But it 
comes with drawbacks. For example, the histogram is not continuous (thus, it 
does not have derivatives); it depends on the position of the origin; and a 
% multivariate	= involving two or more variable quantities.
technique based on histograms is difficuld to generalize to the multivariate 
case. The \emph{native estimator} is a generalization of the histogram 
technique. Since $f$ is the density function of $X$, we have
\begin{equation}\label{eq:naive_estimator}
	f(x) = \lim\limits_{h \rightarrow 0} \frac{1}{2h} \times P(X \in (x-h,x+h))
\end{equation}

where $P(X \in (x-h,x+h))$ is the probability of $X$ falling into $(x-h,x+h)$.
The naive estimator is defined as follows:
\begin{equation}\label{eq:naive_estimator2}
	\hat{f}(x) = \frac{1}{2nh} \times n_i^{\prime}
\end{equation}
where $n_i^{\prime}$ is the number of $X_i s$ falling into $(x-h,x+h)$. 

If $x$ is the center of a bin of the current histogram, then Equation 
\eqref{naive_estimator2} will be equal to Equation \eqref{naive_estimator}. 
Therefore, the naive estimator is a generalization of the histogram technique.
Equation \eqref{naive_estimator2} can also be written as
\begin{equation}\label{eq:naive_estimator3}
	\hat{f}(x) = \frac{1}{nh}\sum_{i=1}^n w\left( \frac{x - X_i}{h} \right)
\end{equation}
where $w$ is defined as 
\begin{equation}\label{naive_estimator3}
	w(x) = \begin{cases}
				1/2 & \text{if } x \in (-1,1) \\
				0 & \text{otherwise }
		\end{cases}
\end{equation}

The naive estimator "inherits" some drawbacks of the histogram technique.
For example, it is not continuous: it "jumps" at points $X_i \pm h$ and has a 
zero derivative for each $x \neq X_i \pm h$.

The naive estimator can be further generalized to a \emph{kernel estimator}. 
Replace function $w$ in Equation \eqref{eq:naive_estimator3} by a function $K$;
we obtain
\begin{equation}\label{eq:kernel_estimator}
	\hat{f}(x) = \frac{1}{nh} \sum_{i=1}^n K\left( \frac{x - X_i}{h} \right)
\end{equation}

where $K$ is requested to satisfy the following condition

\begin{equation}\label{eq:kernel_condition}
	\int_{-\infty}^{\infty} K(x)dx = 1
\end{equation}
because $K(x)$ is a density function.

If $K$ has a derivative, for each $x \in (-\infty, +\infty)$, then by Equation
\eqref{eq:kernel_estimator} so does the kernel estimator. For example, let $K$
be the normal density function, to construct a kernel estimator which has a 
derivative for each $x \in (-\infty, +\infty)$. In Equation \eqref{eq:kernel_estimator}, $h$ is called the \emph{window width (smoothing parameter, or bandwidth); K is called the kernel} function. 
Analogously, we can define the kernel estimator in $\mathbb{R}^d$:
\begin{equation}\label{eq:kernel_estimator_rd}
	\hat{f}(x) = \frac{1}{nh^d} \sum_{i_1}^n K\left( \frac{x-X_i}{h} \right)
\end{equation} 
where $K$ satisfies the following condition
\begin{equation}\label{eq:kernel_rd_condition}
	\int_{R^d} K(x)dx = 1
\end{equation}

In summary, the kernel estimator is an estimator of the density function of a 
random variable, represented by a sum of some simple kernel functions such that
the estimator has "good" mathematical properties such as being differentiable,
symmetric, and so forth.
% subsubsection the_kernel_density_estimator (end)



\subsubsection{The Mean Shift Procedure} % (fold)
\label{ssub:the_mean_shift_procedure}
This subsection explains the principle of \textbf{adaptive?!?} mean shift-based
clustering, meaning that the data points always move to local density maxima.

We review some results of [8]. Let $k(x)$ be a symmetric univariate kernel 
function, where $x \in (-\infty, +\infty)$. We can construct a $dD$ kernel 
function from $k(x)$ as follows:
\begin{equation}\label{eq:dd_kernel}
	K(x)=c_{k,d}k(\lVert x \rVert^2)
\end{equation}
where
\begin{equation}\label{eq:kernel_constant}
	c_{k,d} = \frac{\int_{R^d} K(x) dx} {\int_{R^d} k(\lVert x \rVert^2) dx} = \frac{1}{\int_{R^d} k(\lVert x \rVert^2) dx} > 0
\end{equation}
Function $k(x)$, where $x \in [0, +\infty)$, is called the \emph{profile} of the
kernel $K(x)$. 

By Equation \eqref{eq:dd_kernel}, the kernel density estimator [see Equation \eqref{eq:kernel_estimator_rd}] can be rewritten as follows:

\begin{equation}\label{eq:label}
	\hat{f}_{h,K}(x) = \frac{c_{k,d}}{nh^d}\sum_{i_=1}^n k(\lVert \frac{x-X_i}{h} \rVert^2)
\end{equation}

Suppose that $k(x)$ is differentiable in $[0, +\infty)$, except for a finite 
number of points.
\begin{equation*}\label{eq:shadow_kernel}
	g(x) = -k\prime(x)
\end{equation*}

where $x\in [0, +\infty)$, except for a finite number of points. Construct a 
kernel function from $g(x)$ as follows:
\begin{equation*}\label{eq:shadow_kernel1}
	G(x) = c_{g,d}g(\lVert x \rVert^2)
\end{equation*}

where
\begin{equation}\label{eq:shadow_constant}
	c_{g,d} = \frac{\int_{R^d} G(x) dx} {\int_{R^d} g(\lVert x \rVert^2) dx} = \frac{1}{\int_{R^d} g(\lVert x \rVert^2) dx} > 0
\end{equation}

Denote the gradient of the density estimator of $\hat{f}_{h,K}(x) \text{by} \nabla \hat{f}_{h,K}(x)$. Furthermore (see [8] for the details), let 
\begin{equation}\label{eq:mean_shift0}
	m_{h,G}(x) = \frac{1}{2}h^2 \frac{c_{g,d}}{c_{k,d}} \times \frac{\nabla \hat{f}_{h,K}(x)}{cd_{h,G}(x)}
\end{equation}
where
\begin{equation}\label{eq:mean_shift1}
	m_{h,G}(x) = \frac{\sum_{i=1}^n x_i g\left(\left\lVert \frac{x - x_i}{h} \right\rVert^2\right)}{\sum_{i=1}^n g\left(\left\lVert \frac{x - x_i}{h} \right\rVert^2\right)} -x
\end{equation}

Equation \eqref{eq:mean_shift1} is the difference between the weighted mean and
$x$, known as \emph{mean shift vector}. Since the gradient of the density estimator always points towards that direction in which the density rises most quickliy, by Equation \eqref{eq:mean_shift1}, the mean shift vector always points towards the direction in which the density rises most quickly. This is the main principle of the mean shift based clustering. 
% subsubsection the_mean_shift_procedure (end)


% section theory (end)

The mean shift algorithm is a nonparametric clustering technique which does not
require prior knowledge of the number of clusters, and does not constrain the
shape of the clusters. 

Given $n$ data points $x_i, i = 1, ... , n$ on a
$d$-dimensional space $R^d$, the multivariate kernel density estimate obtained
with kernel $K(x)$ and window radius $h$ is
\begin{equation}\label{density_estimator}
	f(x)=\frac{1}{nh^d}\sum_{i=1}^n K(\frac{x-x_i}{h}).
\end{equation}

For radially symmetric kernels, it suffices to define the profile of the kernel
$k(x)$ satisfying
\begin{equation}
	K(x)=c_{k,d}k(\lVert x \rVert^2)
\end{equation}
where $c_{k,d}$ is a normalization constant which assures $K(x)$ integrates to 
$1$. The modes of the density function are located at the zeros of the gradient
function  $\nabla f(x) = 0$.

The gradient of the density estimator \eqref{density_estimator} is 
\begin{equation}
	\begin{split}
		\nabla f(x) & = \frac{2c_{k,d}}{nh^{d+2}}\sum_{i=1}^n \left(x_i -
        x\right)g\left(\left\lVert \frac{x - x_i}{h} \right\rVert^2\right) \\ &
        = \frac{2c_{k,d}}{nh^{d+2}} \left[ \sum_{i=1}^n g\left(\left\lVert
        \frac{x - x_i}{h} \right\rVert^2\right) \right] \left[
        \frac{\sum_{i=1}^n x_i g\left(\left\lVert \frac{x - x_i}{h}
        \right\rVert^2\right)}{\sum_{i=1}^n g\left(\left\lVert \frac{x - x_i}{h}
        \right\rVert^2\right)} -x \right].
	\end{split}
\end{equation}
where $g(s) = -k'(s)$. The first term is proportional to the density estimate at
$x$ computed with kernel $G(x) = c_{g,d}g(\lvert x \rvert^2)$ and the second 
term
\begin{equation}
	m_h(x) = \frac{\sum_{i=1}^n x_i g\left(\left\lVert \frac{x - x_i}{h} \right\rVert^2\right)}{\sum_{i=1}^n g\left(\left\lVert \frac{x - x_i}{h} \right\rVert^2\right)} -x
\end{equation}
is the \emph{mean shift}. The mean shift vector always points towards the direction of the maximum increase in the density. The mean shift procedure, obtained by successive
\begin{itemize}
	\item computation of the mean shift vector $m_h(x^t)$,
	\item translation of the window $x^{t+1} = x^t + m_h(x^t)$
\end{itemize}
is guaranteed to converge to a point where the gradient of density function is
zero. Mean shift mode finding process is illustrated in \autoref{mean_shift0}.

The mean shift clustering algorithm is a practical application of the mode
finding procedure:
\begin{itemize}
	\item starting on the data points, run mean shift procedure to find the 
	stationary points of the density function,
	\item prune these points by retaining only the local maxima.
\end{itemize}

The set of all locations that converge to the same mode defines the \emph{basin of attraction} of that mode. The points which are in the same basin of 
attraction is associated with the same cluster. \autoref{mean_shift1} shows two examples of mean shift clustering on three dimensional data.





